# LightX2V ä½¿ç”¨ç¤ºä¾‹

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨ LightX2V è¿›è¡Œè§†é¢‘ç”Ÿæˆï¼ŒåŒ…æ‹¬åŸºç¡€ä½¿ç”¨å’Œè¿›é˜¶é…ç½®ã€‚

## ğŸ“‹ ç›®å½•

- [ç¯å¢ƒå®‰è£…](#ç¯å¢ƒå®‰è£…)
- [åŸºç¡€è¿è¡Œç¤ºä¾‹](#åŸºç¡€è¿è¡Œç¤ºä¾‹)
- [æ¨¡å‹è·¯å¾„é…ç½®](#æ¨¡å‹è·¯å¾„é…ç½®)
- [åˆ›å»ºç”Ÿæˆå™¨](#åˆ›å»ºç”Ÿæˆå™¨)
- [è¿›é˜¶é…ç½®](#è¿›é˜¶é…ç½®)
  - [å‚æ•°å¸è½½ (Offload)](#å‚æ•°å¸è½½-offload)
  - [æ¨¡å‹é‡åŒ– (Quantization)](#æ¨¡å‹é‡åŒ–-quantization)
  - [å¹¶è¡Œæ¨ç† (Parallel Inference)](#å¹¶è¡Œæ¨ç†-parallel-inference)
  - [ç‰¹å¾ç¼“å­˜ (Cache)](#ç‰¹å¾ç¼“å­˜-cache)
  - [è½»é‡ VAE (Light VAE)](#è½»é‡-vae-light-vae)

## ğŸ”§ ç¯å¢ƒå®‰è£…

è¯·å‚è€ƒä¸»é¡¹ç›®çš„[å¿«é€Ÿå…¥é—¨æ–‡æ¡£](../docs/ZH_CN/source/getting_started/quickstart.md)è¿›è¡Œç¯å¢ƒå®‰è£…ã€‚

## ğŸš€ åŸºç¡€è¿è¡Œç¤ºä¾‹

æœ€å°åŒ–ä»£ç ç¤ºä¾‹å¯å‚è€ƒ `examples/wan_t2v.py`ï¼š

```python
from lightx2v import LightX2VPipeline

pipe = LightX2VPipeline(
    model_path="/path/to/Wan2.1-T2V-14B",
    model_cls="wan2.1",
    task="t2v",
)

pipe.create_generator(
    attn_mode="sage_attn2",
    infer_steps=50,
    height=480,
    width=832,
    num_frames=81,
    guidance_scale=5.0,
    sample_shift=5.0,
)

seed = 42
prompt = "Your prompt here"
negative_prompt = ""
save_result_path="/path/to/save_results/output.mp4"

pipe.generate(
    seed=seed,
    prompt=prompt,
    negative_prompt=negative_prompt,
    save_result_path=save_result_path,
)
```

## ğŸ“ æ¨¡å‹è·¯å¾„é…ç½®

### åŸºç¡€é…ç½®

å°†æ¨¡å‹è·¯å¾„ä¼ å…¥ `LightX2VPipeline`ï¼š

```python
pipe = LightX2VPipeline(
    model_path="/path/to/Wan2.2-I2V-A14B",
    model_cls="wan2.2_moe",  # å¯¹äº wan2.1ï¼Œä½¿ç”¨ "wan2.1"
    task="i2v",
)
```

### å¤šç‰ˆæœ¬æ¨¡å‹æƒé‡æŒ‡å®š

å½“ `model_path` ç›®å½•ä¸‹å­˜åœ¨å¤šä¸ªä¸åŒç‰ˆæœ¬çš„ bf16 ç²¾åº¦ DIT æ¨¡å‹ safetensors æ–‡ä»¶æ—¶ï¼Œéœ€è¦ä½¿ç”¨ä»¥ä¸‹å‚æ•°æŒ‡å®šå…·ä½“ä½¿ç”¨å“ªä¸ªæƒé‡ï¼š

- **`dit_original_ckpt`**: ç”¨äºæŒ‡å®š wan2.1 å’Œ hunyuan15 ç­‰æ¨¡å‹çš„åŸå§‹ DIT æƒé‡è·¯å¾„
- **`low_noise_original_ckpt`**: ç”¨äºæŒ‡å®š wan2.2 æ¨¡å‹çš„ä½å™ªå£°åˆ†æ”¯æƒé‡è·¯å¾„
- **`high_noise_original_ckpt`**: ç”¨äºæŒ‡å®š wan2.2 æ¨¡å‹çš„é«˜å™ªå£°åˆ†æ”¯æƒé‡è·¯å¾„

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
pipe = LightX2VPipeline(
    model_path="/path/to/Wan2.2-I2V-A14B",
    model_cls="wan2.2_moe",
    task="i2v",
    low_noise_original_ckpt="/path/to/low_noise_model.safetensors",
    high_noise_original_ckpt="/path/to/high_noise_model.safetensors",
)
```

## ğŸ›ï¸ åˆ›å»ºç”Ÿæˆå™¨

### ä»é…ç½®æ–‡ä»¶åŠ è½½

ç”Ÿæˆå™¨å¯ä»¥ä» JSON é…ç½®æ–‡ä»¶ç›´æ¥åŠ è½½ï¼Œé…ç½®æ–‡ä»¶ä½äº `configs` ç›®å½•ï¼š

```python
pipe.create_generator(config_json="../configs/wan/wan_t2v.json")
```

### æ‰‹åŠ¨åˆ›å»ºç”Ÿæˆå™¨

ä¹Ÿå¯ä»¥æ‰‹åŠ¨åˆ›å»ºç”Ÿæˆå™¨ï¼Œå¹¶é…ç½®å¤šä¸ªå‚æ•°ï¼š

```python
pipe.create_generator(
    attn_mode="flash_attn2",  # å¯é€‰: flash_attn2, flash_attn3, sage_attn2, sage_attn3 (Bæ¶æ„æ˜¾å¡é€‚ç”¨)
    infer_steps=50,           # æ¨ç†æ­¥æ•°
    num_frames=81,            # è§†é¢‘å¸§æ•°
    height=480,               # è§†é¢‘é«˜åº¦
    width=832,                # è§†é¢‘å®½åº¦
    guidance_scale=5.0,       # CFGå¼•å¯¼å¼ºåº¦ (=1æ—¶å¼ƒç”¨CFG)
    sample_shift=5.0,         # é‡‡æ ·åç§»
    fps=16,                   # å¸§ç‡
    aspect_ratio="16:9",      # å®½é«˜æ¯”
    boundary=0.900,           # è¾¹ç•Œå€¼
    boundary_step_index=2,    # è¾¹ç•Œæ­¥ç´¢å¼•
    denoising_step_list=[1000, 750, 500, 250],  # å»å™ªæ­¥åˆ—è¡¨
)
```

**å‚æ•°è¯´æ˜ï¼š**
- **åˆ†è¾¨ç‡**: é€šè¿‡ `height` å’Œ `width` æŒ‡å®š
- **CFG**: é€šè¿‡ `guidance_scale` æŒ‡å®šï¼ˆè®¾ç½®ä¸º 1 æ—¶ç¦ç”¨ CFGï¼‰
- **FPS**: é€šè¿‡ `fps` æŒ‡å®šå¸§ç‡
- **è§†é¢‘é•¿åº¦**: é€šè¿‡ `num_frames` æŒ‡å®šå¸§æ•°
- **æ¨ç†æ­¥æ•°**: é€šè¿‡ `infer_steps` æŒ‡å®š
- **é‡‡æ ·åç§»**: é€šè¿‡ `sample_shift` æŒ‡å®š
- **æ³¨æ„åŠ›æ¨¡å¼**: é€šè¿‡ `attn_mode` æŒ‡å®šï¼Œå¯é€‰ `flash_attn2`, `flash_attn3`, `sage_attn2`, `sage_attn3`ï¼ˆBæ¶æ„æ˜¾å¡é€‚ç”¨ï¼‰

## âš™ï¸ è¿›é˜¶é…ç½®

**âš ï¸ é‡è¦æç¤ºï¼šæ‰‹åŠ¨åˆ›å»ºç”Ÿæˆå™¨æ—¶ï¼Œå¯ä»¥é…ç½®ä¸€äº›è¿›é˜¶é€‰é¡¹ï¼Œæ‰€æœ‰è¿›é˜¶é…ç½®å¿…é¡»åœ¨ `create_generator()` ä¹‹å‰æŒ‡å®šï¼Œå¦åˆ™ä¼šå¤±æ•ˆï¼**

### å‚æ•°å¸è½½ (Offload)

æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼Œå‡ ä¹ä¸å½±å“æ¨ç†é€Ÿåº¦ï¼Œé€‚ç”¨äº RTX 30/40/50 ç³»åˆ—æ˜¾å¡ã€‚

```python
pipe.enable_offload(
    cpu_offload=True,              # å¯ç”¨ CPU å¸è½½
    offload_granularity="block",   # å¸è½½ç²’åº¦: "block" æˆ– "phase"
    text_encoder_offload=False,    # æ–‡æœ¬ç¼–ç å™¨æ˜¯å¦å¸è½½
    image_encoder_offload=False,   # å›¾åƒç¼–ç å™¨æ˜¯å¦å¸è½½
    vae_offload=False,             # VAE æ˜¯å¦å¸è½½
)
```

**è¯´æ˜ï¼š**
- å¯¹äº Wan æ¨¡å‹ï¼Œ`offload_granularity` æ”¯æŒ `"block"` å’Œ `"phase"`
- å¯¹äº HunyuanVideo-1.5ï¼Œç›®å‰åªæ”¯æŒ `"block"`

### æ¨¡å‹é‡åŒ– (Quantization)

é‡åŒ–å¯ä»¥æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿæ¨ç†ã€‚

```python
pipe.enable_quantize(
    dit_quantized=False,                    # æ˜¯å¦ä½¿ç”¨é‡åŒ–çš„ DIT æ¨¡å‹
    text_encoder_quantized=False,           # æ˜¯å¦ä½¿ç”¨é‡åŒ–çš„æ–‡æœ¬ç¼–ç å™¨
    image_encoder_quantized=False,          # æ˜¯å¦ä½¿ç”¨é‡åŒ–çš„å›¾åƒç¼–ç å™¨
    dit_quantized_ckpt=None,                # DIT é‡åŒ–æƒé‡è·¯å¾„ï¼ˆå½“ model_path ä¸‹æ²¡æœ‰é‡åŒ–æƒé‡æˆ–å­˜åœ¨å¤šä¸ªæƒé‡æ—¶éœ€è¦æŒ‡å®šï¼‰
    low_noise_quantized_ckpt=None,          # Wan2.2 ä½å™ªå£°åˆ†æ”¯é‡åŒ–æƒé‡è·¯å¾„
    high_noise_quantized_ckpt=None,         # Wan2.2 é«˜å™ªå£°åˆ†æ”¯é‡åŒ–æƒé‡è·¯å¾„
    text_encoder_quantized_ckpt=None,       # æ–‡æœ¬ç¼–ç å™¨é‡åŒ–æƒé‡è·¯å¾„ï¼ˆå½“ model_path ä¸‹æ²¡æœ‰é‡åŒ–æƒé‡æˆ–å­˜åœ¨å¤šä¸ªæƒé‡æ—¶éœ€è¦æŒ‡å®šï¼‰
    image_encoder_quantized_ckpt=None,      # å›¾åƒç¼–ç å™¨é‡åŒ–æƒé‡è·¯å¾„ï¼ˆå½“ model_path ä¸‹æ²¡æœ‰é‡åŒ–æƒé‡æˆ–å­˜åœ¨å¤šä¸ªæƒé‡æ—¶éœ€è¦æŒ‡å®šï¼‰
    quant_scheme="fp8-sgl",                 # é‡åŒ–æ–¹æ¡ˆ
)
```

**å‚æ•°è¯´æ˜ï¼š**
- **`dit_quantized_ckpt`**: å½“ `model_path` ç›®å½•ä¸‹æ²¡æœ‰é‡åŒ–æƒé‡ï¼Œæˆ–å­˜åœ¨å¤šä¸ªæƒé‡æ–‡ä»¶æ—¶ï¼Œéœ€è¦æŒ‡å®šå…·ä½“çš„ DIT é‡åŒ–æƒé‡è·¯å¾„
- **`text_encoder_quantized_ckpt`** å’Œ **`image_encoder_quantized_ckpt`**: ç±»ä¼¼åœ°ï¼Œç”¨äºæŒ‡å®šç¼–ç å™¨çš„é‡åŒ–æƒé‡è·¯å¾„
- **`low_noise_quantized_ckpt`** å’Œ **`high_noise_quantized_ckpt`**: ç”¨äºæŒ‡å®š Wan2.2 æ¨¡å‹çš„åŒåˆ†æ”¯é‡åŒ–æƒé‡

**é‡åŒ–æ¨¡å‹ä¸‹è½½ï¼š**

- **Wan-2.1 é‡åŒ–æ¨¡å‹**: ä» [Hy1.5-Quantized-Models](https://huggingface.co/lightx2v/Wan2.1-Distill-Models) ä¸‹è½½
- **Wan-2.2 é‡åŒ–æ¨¡å‹**: ä» [Hy1.5-Quantized-Models](https://huggingface.co/lightx2v/Wan2.2-Distill-Models) ä¸‹è½½
- **HunyuanVideo-1.5 é‡åŒ–æ¨¡å‹**: ä» [Hy1.5-Quantized-Models](https://huggingface.co/lightx2v/Hy1.5-Quantized-Models) ä¸‹è½½
  - `hy15_qwen25vl_llm_encoder_fp8_e4m3_lightx2v.safetensors` æ˜¯æ–‡æœ¬ç¼–ç å™¨çš„é‡åŒ–æƒé‡

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# HunyuanVideo-1.5 é‡åŒ–ç¤ºä¾‹
pipe.enable_quantize(
    quant_scheme='fp8-sgl',
    dit_quantized=True,
    dit_quantized_ckpt="/path/to/hy15_720p_i2v_fp8_e4m3_lightx2v.safetensors",
    text_encoder_quantized=True,
    image_encoder_quantized=False,
    text_encoder_quantized_ckpt="/path/to/hy15_qwen25vl_llm_encoder_fp8_e4m3_lightx2v.safetensors",
)

# Wan2.1 é‡åŒ–ç¤ºä¾‹
pipe.enable_quantize(
    dit_quantized=True,
    dit_quantized_ckpt="/path/to/wan2.1_i2v_480p_scaled_fp8_e4m3_lightx2v_4step.safetensors",
)

# Wan2.2 é‡åŒ–ç¤ºä¾‹
pipe.enable_quantize(
    dit_quantized=True,
    low_noise_quantized_ckpt="/path/to/wan2.2_i2v_A14b_low_noise_scaled_fp8_e4m3_lightx2v_4step.safetensors",
    high_noise_quantized_ckpt="/path/to/wan2.2_i2v_A14b_high_noise_scaled_fp8_e4m3_lightx2v_4step_1030.safetensors",
)
```

**é‡åŒ–æ–¹æ¡ˆå‚è€ƒï¼š** è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ [é‡åŒ–æ–‡æ¡£](../docs/ZH_CN/source/method_tutorials/quantization.md)

### å¹¶è¡Œæ¨ç† (Parallel Inference)

æ”¯æŒå¤š GPU å¹¶è¡Œæ¨ç†ï¼Œéœ€è¦ä½¿ç”¨ `torchrun` è¿è¡Œï¼š

```python
pipe.enable_parallel(
    seq_p_size=4,                    # åºåˆ—å¹¶è¡Œå¤§å°
    seq_p_attn_type="ulysses",       # åºåˆ—å¹¶è¡Œæ³¨æ„åŠ›ç±»å‹
)
```

**è¿è¡Œæ–¹å¼ï¼š**
```bash
torchrun --nproc_per_node=4 your_script.py
```

### ç‰¹å¾ç¼“å­˜ (Cache)

å¯ä»¥æŒ‡å®šç¼“å­˜æ–¹æ³•ä¸º Mag æˆ– Teaï¼Œä½¿ç”¨ MagCache å’Œ TeaCache æ–¹æ³•ï¼š

```python
pipe.enable_cache(
    cache_method='Tea',  # ç¼“å­˜æ–¹æ³•: 'Tea' æˆ– 'Mag'
    coefficients=[-3.08907507e+04, 1.67786188e+04, -3.19178643e+03,
                  2.60740519e+02, -8.19205881e+00, 1.07913775e-01],  # ç³»æ•°
    teacache_thresh=0.15,  # TeaCache é˜ˆå€¼
)
```

**ç³»æ•°å‚è€ƒï¼š** å¯å‚è€ƒ `configs/caching` æˆ– `configs/hunyuan_video_15/cache` ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶

### è½»é‡ VAE (Light VAE)

ä½¿ç”¨è½»é‡ VAE å¯ä»¥åŠ é€Ÿè§£ç å¹¶é™ä½æ˜¾å­˜å ç”¨ã€‚

```python
pipe.enable_lightvae(
    use_lightvae=False,    # æ˜¯å¦ä½¿ç”¨ LightVAE
    use_tae=False,         # æ˜¯å¦ä½¿ç”¨ LightTAE
    vae_path=None,         # LightVAE çš„è·¯å¾„
    tae_path=None,         # LightTAE çš„è·¯å¾„
)
```

**æ”¯æŒæƒ…å†µï¼š**
- **LightVAE**: ç›®å‰åªæ”¯æŒ wan2.1ã€wan2.2 moe
- **LightTAE**: ç›®å‰åªæ”¯æŒ wan2.1ã€wan2.2-ti2vã€wan2.2 moeã€HunyuanVideo-1.5

**æ¨¡å‹ä¸‹è½½ï¼š** è½»é‡ VAE æ¨¡å‹å¯ä» [Autoencoders](https://huggingface.co/lightx2v/Autoencoders) ä¸‹è½½

- Wan-2.1 çš„ LightVAE: [lightvaew2_1.safetensors](https://huggingface.co/lightx2v/Autoencoders/blob/main/lightvaew2_1.safetensors)
- Wan-2.1 çš„ LightTAE: [lighttaew2_1.safetensors](https://huggingface.co/lightx2v/Autoencoders/blob/main/lighttaew2_1.safetensors)
- Wan-2.2-ti2v çš„ LightTAE: [lighttaew2_2.safetensors](https://huggingface.co/lightx2v/Autoencoders/blob/main/lighttaew2_2.safetensors)
- HunyuanVideo-1.5 çš„ LightTAE: [lighttaehy1_5.safetensors](https://huggingface.co/lightx2v/Autoencoders/blob/main/lighttaehy1_5.safetensors)

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# ä½¿ç”¨ HunyuanVideo-1.5 çš„ LightTAE
pipe.enable_lightvae(
    use_tae=True,
    tae_path="/path/to/lighttaehy1_5.safetensors",
    use_lightvae=False,
    vae_path=None
)
```

## ğŸ“š æ›´å¤šèµ„æº

- [å®Œæ•´æ–‡æ¡£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/)
- [GitHub ä»“åº“](https://github.com/ModelTC/LightX2V)
- [HuggingFace æ¨¡å‹åº“](https://huggingface.co/lightx2v)
